name: Automate Scraping and Database Update

on:
  schedule:
    - cron: '0 9 * * *'  # Run daily at midnight UTC (you can adjust the schedule here)
  workflow_dispatch:  # Allow manual triggering from GitHub UI

jobs:
  update-database:
    runs-on: ubuntu-latest  # Run the job on the latest Ubuntu server
    steps:
      - name: Checkout the repository
        uses: actions/checkout@v2

      - name: Set up Python (if required for the script)
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'  # Use the appropriate Python version

      - name: Install dependencies (if needed)
        run: |
          sudo apt-get update
          sudo apt-get install -y python3-pip  # If using Python, install pip
          pip3 install -r requirements.txt  # Install any Python dependencies

      - name: Print working directory (debugging)
        run: |
          pwd

      - name: Run scraping script
        run: |
          python3 public_html/resources/data/update_database.py  # Adjust the path to your script
        env:
          DB_HOST: ${{ secrets.DB_HOST }}  # Use secrets for sensitive information like DB credentials
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_NAME: ${{ secrets.DB_NAME }}
